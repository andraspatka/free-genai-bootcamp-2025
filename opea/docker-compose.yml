services:
  ollama-server:
    image: ollama/ollama
    container_name: ollama-server
    ports:
      - ${LLM_ENDPOINT_PORT:-8008}:11434
    environment:
      LLM_MODEL_ID: "llama3.2:1b" # Doesnt' do anything??
      host_ip: ${host_ip}
  
    command: --model-id ${LLM_MODEL_ID} --cuda-graphs 0

networks:
  default:
    driver: bridge